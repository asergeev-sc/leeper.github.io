# Reproducible Research #

I recently assigned an article from *The American Political Science Review* to the students in my Master-level [Quantitative Political Analysis](http://thomasleeper.com/regcourse/) course. I'd chosen the article because it implemented a technique I wanted to demonstrate and the data and code for the project were publicly available online in [The Harvard IQSS Dataverse](http://thedata.org/), an archive for social science data and associated analysis. In prepping the class, I tried to rerun the analysis files for the study and ultimately I wasn't able - even with the data and the code in-hand - reproduce the published results. As an advocate for reprodudible research, this made me wonder what hope there is for open science and science writ-large if this is the reality of today's scientific enterprise.

I continue to find that practicing scientists have never thought about open science or reproducibility. In my discipline, most work is done in a proprietary statistical application called [Stata](http://www.stata.com/). It's a decent program that has nice defaults and makes it very easy to learn and use common statistical techniques. But Stata is expensive, hundreds of dollars expensive, and new releases mean researchers have to buy new licenses. I have a perpetual license to Stata 8, which I bought during graduate school, but this software will never be able to do matching, multilevel modelling, or multiple imputation - techniques that are essentially required in the toolbox of modern social science. If I want to implement them in Stata, I need to pay money. For a reproducibility perspective, this also means that any research I published that uses Stata will require them to buy Stata, too. And yet, if I run my analyses in Stata 8 because that's what I have on hand and someone else wants to reproduce by results using Stata 13 (or some future edition) it is entirely possible that they won't be able to reproduce the results because Stata changes its API. Since previous versions of Stata are unavailable, all that connects the data I used to the claims I made in the published work is my good word. I think my word is pretty credible and reliable, why should anyone else believe?

This is the question that reproducible research aims to address. Scientists should be skeptical of everything they read, so when I publish research the data and analysis files that I publish are my way of overcoming that skepticism. Publicly archiving those files is a signal that means "don't just take my word, have a look yourself." But in the opening anecdote, I noted that I couldn't reproduce the published results even with these files. So should I disbelieve the results in the published article? Well, maybe. When I tried to run the code in R, it became apparent that the code didn't reproduce because it relied on a package called [Zelig](http://cran.r-project.org/web/packages/Zelig/index.html) created by Gary King (the elder statesman of political science replication) and various collaborators. The article I was looking at used Zelig in R v2.0 and since that time R has change (we're currently on v3.1) and the Zelig API had changed, so the functions no longer worked with the current version of Zelig. The authors didn't mark up their code enough for me to try to reproduce their results using the current Zelig API and the Zelig documentation doesn't really describe what changed. Fortunately for science, I could go back to [CRAN](http://cran.r-project.org/) and download copies of R 2.0 and the version of Zelig available from that (but [recent discussions show even that can be difficult](http://r.789695.n4.nabble.com/RFC-A-case-for-freezing-CRAN-td4687072.html)). The weakness here - as when using Stata - is that even if an author tries to do everything in their power to share reproducible results, those results are only reproducible to the extent that the underlying software necessary to replicate them remains easily available.

But where does that leave us? Is there no hope for easy reproducibility? No, I think making reproducible research can be easy, but it requires changing the way we think about research practice and scientific publication. Ultimately, in order to make our scientific efforts reproducible, we need to think like the future users of our data. What information and resources do they need available to them in order to reproduce our results? Here's a short list:

## 1. The data ##
Ideally, you can distribute your data publicly through somewhere like Dataverse or reference a publicly available dataset available from a persistent archive, like [ICPSR](http://www.icpsr.umich.edu/icpsrweb/landing.jsp). A senior scholar recently told me that all of his data are online and therefore easy to access. I told him they were only archived on his website to which he replied, "Well, the only problem then is if I die." When you publish your data in a persistent archive, others hoping to look at your data don't have to worry about whether you're dead.

And, good practice is to [cite the specific version of the data you use](http://thedata.org/citation) in case changes are made to the dataset after you use it (e.g., because errata are published on the original dataset). Data should also be in an open format. Comma-separated value files are an obvious choice, Stata, SAS, and SPSS files should be *verboten*. (Note, for example, that [R will not read Stata files beyond version 11](http://cran.r-project.org/web/packages/foreign/index.html).)

## 2. A codebook ##

By moving your data to file format like CSV, you necessarily lose all metadata - the long-form variable descriptions and possibly variable labels that help users make sense of your data. I was taught in graduate school to write a Word (.doc) file containing all of this. Recommended practice, however, is to create a [Data Documentation Initiative (DDI) file](http://www.ddialliance.org/). Creating one my hand is tedious, but programs like [SledgeHammer](http://www.openmetadata.org/) or [Colectica](http://www.colectica.com/) will make them for you. Dataverse also generates DDI automatically.

## 3. The complete analysis files ##

These files should produce every number, table, and graphic in the published article. Most scientists have such files on hand, but making them usable to others requires ensuring that the files are complete (i.e., contain every analysis), are organized in a comprehensible way, and are carefully documented. On this last point, [Jake Bowers](http://www.jakebowers.org/) has [a terrific article describing how to document code](http://polmeth.wustl.edu/methodologist/tpm_v18_n2.pdf).
 
## 4. A README ##
This helps the end-user make sense of the complete package of archived materials. Sharing data and code is great, but it can be hard to make sense of because each of uses different coding styles and may rely on obscure functions or packages. Describing how the files yield the published results can be really helpful.

## 5. A record of versioned software used ##
Software changes all the time and those changes might be consequential for reproducibility. It might seem weird to write down that you produced your results on May 4, 2014 using Windows 7, R v3.1.0, and specific versions of add-on packages, but this information could be vital to reproducibility. This information could be in the README, the code, and/or the published article. Fully citing it in the article has the advantage of giving appropriate credit to scientists who often thanklessly produce the software we use everyday.

## Conclusion ##

This might all seem like a lot of work, but distributing a package of reproducible research files will make you a better scientist and make your scientific output more credible. When you think about this as part of the output of the scientific process, you'll work better. For example, when you focus on documenting the production of each table and figure in your paper, you'll learn to stop using programs like Excel or other Graphical User Interfaces to produce results because they are fundamentally not reproducible. You'll also help yourself out, If you ever end up being the future end-user of your own data, you'll appreciate having a complete package of files that are easy to make sense of. And putting all of this material out for the world to see should help you feel confident in your results. Even if someone challenges your findings, at least you've been honest about how you came to your results. It's an honorable part of science to have your work challenged; it's fundamentally dishonest to hide your data and methods.
 
