# It's there unless I die: Why reproducibility requires data archiving #

In a recent conversation with a senior scholar in political science, I brought up the issue of reproducibility and the need for open access to the data associated with published scientific articles. The topic was provoked by the recent symposium on data access and research transparency (DART) that this blog has covered extensively. I pointed out that this scholar's data were not available in a persistent data archive. Their response was that the data were available on their academic website and "the data would be there unless I die".

I appreciate when scholars make their data available. Putting it on a personal website is an easy way of sharing data, but it is ultimately suboptimal. The data sharing strategy most convenient for a data author is rarely the strategy most convenient for the data end-user. And while thinking about reproducibility on the timescale of one's lifetime seems prudent, thinking about data use beyond that horizon is even better because it means that scientific contributions wrought today can bring value to scientific inquiry long in to the future. How can we make lasting contributions of scientific data? Basically, we need to (1) put data somewhere that will last and (2) document it in a way that makes it easy for others to use. Both of these steps are actually easy but they require you to think a little bit about what it might be like for a future scientist to access your data and analysis code. This future scientist may have different software, different hardware, different training, and different research goals from you as the data creator. And, you may already be dead.

**Where should we archive our data?**

This part is easy. You need to put data somewhere it will last. For decades, the go-to place for persistently storing social science data has been [ICPSR](http://www.icpsr.umich.edu/icpsrweb/landing.jsp). That remains a great option, but there are always new options popping up. [Dryad](http://datadryad.org/), [figshare](http://figshare.com/), and archives running The Dataverse Network archiving software, including [the Harvard IQSS Dataverse Network](http://thedata.harvard.edu/dvn/) and UNC-Chapel Hill's [Odum Institute](http://arc.irss.unc.edu/dvn/).

All of these services provide a persistent data archive, meaning that archived data are promised to be available as long as the hosting institution can support it. And, all are backed up by [LOCKSS](http://www.lockss.org/) a Stanford-created distributed network of backups that mean data are safe even if the original server hosting the data is physically destroyed. Persistent data archives promise a lifespan for your data that far exceeds your own.

These sites also share other nice properties. They provide [DOIs](http://en.wikipedia.org/wiki/Digital_object_identifier), the persistent pointer to your data regardless of where it lives on the internet. This makes data [citable](http://thedata.org/citation) and Dataverse, in particular, stores data under version control so that corrections can be made to published data while still preserving the replicability of results based on earlier iterations. All three sites also integrate beautifully with the open source statistical software R (due to the work of [the rOpenSci project](http://ropensci.org/)), making it ease to archive data as part of the usual scientific workflow.

With such great online resources available, there's no reason to distribute data on one's own website. Your data are in better hands at any of these sites.


**What should we archive?**

In order to make our scientific efforts reproducible, we need to think like the future users of our data. What information and resources do they need available to them in order to reproduce our results? It's easy to imagine yourself in that position. Just think about the last time you want to look at someone else's data and replicate their results. (If you've never been in that situation, just try it sometime for fun.) Was the data in an open science format (as opposed to stored in a way that you couldn't understand or couldn't open)? Was there a codebook describing what the data actually were? Was there a complete code file containing the code that translated data into published analyses? Was there a record of what versions of software were used in those analyses? If you've ever been in this situation, a "no" answer to any of these questions likely caused problems for you in replicating the original analysis or using the data to other ends.

Once you've experienced those challenges as a data user, you know that they should guide your behavior as a data producer. Just putting the Stata file (version unknown) of your data on your website without any supplemental information means not only that others may have difficult understanding it but also that in the future users may be unable to open it. Ease for the data creator is inversely proportional to its utility for end user.

Here's what a rough outline of what your persistently archived data package should contain:

## 1. Data ##
Data needs to be shared in an open scientific format. Most political science data is simple and rectangular, so comma-separated value (CSV) or tab-separated value (TSV) files are the obvious choice. For larger datasets, fixed-width formats tradeoff ease of use with file size; binary formats like [NetCDF](http://en.wikipedia.org/wiki/NetCDF) might also work in specific applications. Stata, SAS, SPSS, and Excel files should be *verboten*. (Note, for example, that [R will not read Stata files beyond version 11](http://cran.r-project.org/web/packages/foreign/index.html).) Beyond sharing your data, good practice is to [cite the specific version of the data you use](http://thedata.org/citation) in case changes are made to the dataset after you use it (e.g., because errata are published on the original dataset).

## 2. Metadata ##
By moving your data to a file format like CSV, you necessarily lose all metadata - the long-form variable descriptions and possibly variable labels that help users make sense of your data. I was taught in graduate school to write a Word (.doc) file containing all of this. A plain text file would probably be better. Recommended current practice, however, is to create a [Data Documentation Initiative (DDI) file](http://www.ddialliance.org/). Creating one my hand is tedious, but programs like [SledgeHammer](http://www.openmetadata.org/) or [Colectica](http://www.colectica.com/) will make them for you. Dataverse also generates DDI automatically when data are archived.

## 3. Analysis ##
If you're publishing replication data, you need to include analysis. These files should produce every number, table, and graphic in the published article. Most scientists have such files on hand, but making them usable to others requires ensuring that the files are complete (i.e., contain every analysis, with no point-and-click operations), are organized in a comprehensible way (i.e., it shouldn't be 10,000 lines of code in one file), and are carefully documented to describe what you did. On this last point, [Jake Bowers](http://www.jakebowers.org/) has [a terrific article describing how to document code](http://polmeth.wustl.edu/methodologist/tpm_v18_n2.pdf). Even if your code doesn't work in the future, your comments should supply enough information for others to recreate what you did from scratch in the language of their choice.
 
## 4. README ##
A README helps the end-user make sense of the complete package of archived materials. Sharing data and code is great, but it can be hard to make sense of because each of us uses different coding styles, different software, and different scientific workflow. Describing how the complete package of data files come together to yield the published results can be really helpful.

## 5. Software ##
Software changes all the time and those changes might be consequential for reproducibility. It might seem weird to write down that you produced your results on May 4, 2014 using Windows 7, R v3.1.0, and specific versions of add-on packages, but this information could be vital to reproducibility. This information could be in the README, the code, and/or your original published article. Fully citing software in the article has the nice advantage of giving appropriate credit to scientists who often thanklessly produce the software we use everyday.

**Conclusion**

This might all seem like a lot of work, but distributing a package of reproducible research files will make you a better scientist and make your scientific output more credible and longer lasting. When you think about data archiving as part of the output of the scientific process, you'll work better. For example, when you focus on documenting the production of each table and figure in your paper, you'll learn to stop using programs like Excel or other graphical user interfaces to produce results because they are fundamentally not reproducible. You'll also help yourself out: if you ever find yourself as the future end-user of your own data, you'll appreciate having a complete package of files that are easy to make sense of. And putting all of this material out for the world to see should help you feel confident in your results. Even if someone challenges your findings, at least you've been honest about how you came to your results and been willing to let others have a look. It's an honorable part of science to have your work challenged; it's fundamentally dishonest to hide your data and methods. And if you've made your data open and analyses reproducible in the long-term by storing them in a persistent data archive, your data and code will be there to defend your work even if you're long gone.
 
